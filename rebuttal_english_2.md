# Rebuttal to Reviewer JwJA

We thank the reviewer for the careful reading and valuable suggestions. We provide our feedback as follows.

## **Weakness 1: Limited evaluation, lack of multi-random seed experiments**

Thank you for raising this important question. In fact, we have already reported experiments based on 4 random seeds in Supplementary Material C.1 and shown success rates with error bars in Figure 1. The results demonstrate that both the original WebGRPO and the enhanced version WebGRPO (+Skills) significantly outperform all baseline methods across two model families. Additionally, we have further supplemented experiments using 5 random seeds to further validate the stability of our method. Below are the average success rates of each method on WebArena-Lite:

- **GLM-4:** SFT 22.2%, Filtered BC 24.5%, AWR 27.6%, DigiRL 31.4%, WEBRL 43.4%, WebGRPO 50.6%, WebGRPO (+Skills) 58.2%.
- **LLaMA3.1:** SFT 20.3%, Filtered BC 23.1%, AWR 28.4%, DigiRL 30.0%, WEBRL 41.9%, WebGRPO 51.6%, WebGRPO (+Skills) 59.7%.

The results further validate that our method performs stably across multiple random initializations and achieves significant performance improvements.

## **Weakness 2: Lack of base model scale analysis**

Thank you for pointing out this issue. To further analyze the impact of base model scale on performance, we supplemented experiments on the Qwen2.5-3B model and evaluated using 5 random seeds. The average success rates are: Reddit 36.3%, GitLab 51.2%, CMS 47.3%, Map 32.3%, OSS 28.1%, with an overall average of 39.0%, indicating that our method remains effective on smaller models. In the future, we plan to conduct further exploration on Llama3.1-70B scale models to systematically evaluate the impact of scale expansion on performance.

## **Weakness 3: Reward/evolution model scale analysis is similarly limited**

Thank you for your attention to the RXERM reward mechanism evaluation. We clarify that RXERM includes two parts: execution reward (judging whether the task is completed) and reasoning reward (evaluating the quality of intermediate reasoning steps). We have conducted preliminary validation of GPT-4o as an evaluator in early stages. Specifically, we randomly sampled 100 trajectories across multiple tasks and compared GPT-4o's judgments with human annotations and other evaluation models (Claude, DeepSeek-R1, etc.). The results show that GPT-4o has high consistency with human annotations (execution reward >90%, reasoning reward >85%) and very small differences with other strong evaluators (<3%). Based on this stability result and considering that we have free access to GPT-4o's internal company API, from cost and resource perspectives, we ultimately chose GPT-4o as the RXERM evaluator. Additionally, regarding the performance impact of SkillGenesis under different LLMs, we have provided systematic analysis in Supplementary Material C.4 "Performance Comparison of SkillEvo with Different LLMs". In response to the reviewer's concerns, we will further supplement experiments in the revision: using multiple open-source evaluation models to replace GPT-4o, conducting comparative analysis of RXERM's execution reward accuracy and reasoning reward reliability to verify the robustness of the method.

## **Weakness 4: Lack of dataset and final model training details**

Thank you for raising this question. **Regarding the dataset:** The dataset used for SFT training is a human-annotated demonstration dataset provided by WebArena-Lite. This dataset was carefully designed and generated by 5 authors based on approximately 40 task prototypes, ultimately constructing 1,186 high-quality expert trajectories. These demonstrations cover 5 real websites in WebArena-Lite with diverse task types and clear operation steps. For training data in the reinforcement learning phase, the Baselines section in Section 3.1 of the main text has clear descriptions. We adopted automatic task generation, specifically, instructions come from a skill proposer. This module automatically generates numerous task instances based on initial webpage states during training, forming a task instance pool. Specific details can be found in *Skill Proposal Stage* in Section 2.3 *SkillGenesis*. **Regarding training details:** We have actually provided relevant information in Supplementary Material B (Training Details), including training hyperparameters, steps, mini batch size, etc. The training cost of the SkillEvo framework mainly comes from two stages: WebGRPO's trajectory-level optimization training and SkillGenesis's skill evolution exploration. **In the training phase**, WebGRPO used 8×H100 GPUs for approximately 38 hours of training, and SkillGenesis's exploration phase conducted 160 iterations, taking approximately 2.5 hours, with total training GPU hours of approximately 324 GPU hours. **In terms of query costs,** WebGRPO uses RXERM for trajectory reward modeling, requiring 1+T GPT-4o calls per trajectory (where T is the number of reasoning steps, including T reasoning evaluations and 1 execution evaluation); the SkillGenesis stage triggers one GPT-4o skill evolving call every 20 iterations. Based on the overall process estimation, the total call volume is approximately 101,000 GPT-4o calls. Among these, RXERM two-stage reward evaluation accumulates approximately 248M tokens (execution reward approximately 220M tokens, reasoning reward approximately 28M tokens). Additional overhead mainly comes from SkillGenesis bringing approximately 42M tokens (skill proposal one-time approximately 64k tokens, skill genesis approximately 42M tokens, skill evolving approximately 50k tokens).

## **Weakness 5: Lack of failure case analysis**

Thank you for the valuable suggestion regarding failure case analysis. We have not yet systematically discussed failed tasks in the paper, which is one of the key directions for our future work. In future versions, we plan to supplement attribution analysis of typical failure cases, such as missing skill calls and parameter filling errors, to further reveal the potential limitations of the method in complex tasks and provide directional improvements for skill structure optimization and strategy generalization.

Based on current preliminary analysis, we found that after WebGRPO training, the model's **skill calling ability** declined in some tasks, reflecting a disconnect between strategy execution and skill calling. Even when appropriate skills can be correctly identified during the exploration phase, the model fails to call them during actual execution, instead relying on the execution of atomic webpage operations. For example, in the "view details of a book" task, the model should call the `search_books_by_title('...')` API but actually doesn't call it. This phenomenon is not just a misjudgment of calling timing, but more fundamentally: the current strategy model lacks global evaluation capability for skill calling value, and skill knowledge has not been effectively integrated into the strategy model, leading to inappropriate timing choices for calling or even being ignored. Another common problem is **parameter filling errors**, where the model correctly selects the API to call but has semantic understanding bias in parameter construction, especially when handling complex constraints. For example, in the "find recipes containing broccoli but no dairy products" task, the model incorrectly generates `search_recipes_by_ingredients('broccoli, milk-free')`, while the correct call should be `search_recipes_by_ingredients('broccoli', exclude='dairy')`. Further analysis reveals that such errors mostly stem from the model's insufficient understanding of fine-grained instruction constraints, inability to correctly parse negative conditions and exclusionary constraints, resulting in information loss or misunderstanding during the skill parameterization process.

## **Question: Plot Readability and Table 1 Annotation**

Thank you for your suggestion. We will increase the font size in all plots (excluding schemes) to improve readability, and we will adjust Table 1 to use **bold** for the best results and *underline* (or other convention) for the second-best results in the revision.

---

**We hope the above supplements and clarifications can resolve the reviewer's concerns, and we would be more grateful if you could improve the score.**

# Rebuttal to Reviewer 1HdP

We thank the reviewer for the careful reading and valuable suggestions. We provide our feedback as follows.

## **Weakness 1: Reward mechanism RXERM not sufficiently evaluated**

Thank you for your attention to the RXERM reward mechanism evaluation. We clarify that RXERM includes two parts: execution reward (judging whether the task is completed) and reasoning reward (evaluating the quality of intermediate reasoning steps). We have conducted preliminary validation of GPT-4o as an evaluator in early stages. Specifically, we randomly sampled 100 trajectories across multiple tasks and compared GPT-4o's judgments with human annotations and other evaluation models (Claude, DeepSeek-R1, etc.). The results show that GPT-4o has high consistency with human annotations (execution reward >90%, reasoning reward >85%) and very small differences with other strong evaluators (<3%). Based on this stability result and considering that we have free access to GPT-4o's internal company API, from cost and resource perspectives, we ultimately chose GPT-4o as the RXERM evaluator.  In response to the reviewer's concerns, we will further supplement experiments in the revision: using multiple open-source evaluation models to replace GPT-4o, conducting comparative analysis of RXERM's execution reward accuracy and reasoning reward reliability to verify the robustness of the method.

## **Weakness 2: Disclose tasks used for model training. **

The dataset used for SFT training is a human-annotated demonstration dataset provided by WebArena-Lite. This dataset was carefully designed and generated by 5 authors based on approximately 40 task prototypes, ultimately constructing 1,186 high-quality expert trajectories. These demonstrations cover 5 real websites in WebArena-Lite with diverse task types and clear operation steps. For training data in the reinforcement learning phase, the Baselines section in Section 3.1 of the main text has clear descriptions. We adopted automatic task generation, specifically, instructions come from a skill proposer. This module automatically generates numerous task instances based on initial webpage states during training, forming a task instance pool. Specific details can be found in *Skill Proposal Stage* in Section 2.3 *SkillGenesis*.

## **Weakness 3: Definition of skills in SkillGenesis and details about SPG**

In the SkillGenesis framework, "Skill" refers to reusable executable Python programs that abstract multiple low-level webpage interaction actions (such as clicking, typing, scrolling, etc.) into a higher-level function call.

The skill library stores "executable skill functions (skill units)", while SPG stores structural relationships between skills, including calling sequences and dependency paths.

In fact, the definition of SPG has been clearly stated in Section 2.3 SkillGenesis. SPG is a directed graph structure used to organize the structure and dependencies between skills, where each node represents a skill, and each edge represents the calling sequence or dependency relationship between skills. Its structure is stored in JSON, consisting of two core fields: `nodes` and `edges`. `nodes` represents the skill set, `edges` represents structural relationships between skills, recording two key connections between skills: In these two types of relationships, "dependency relationship" (`depends_on`) is a strong constraint, indicating that the execution of the target skill semantically or state-wise must depend on the completion of the source skill, for example: "SearchItem" depends on the user having completed the "Login" operation to execute; while "calling sequence" (`invokes`) is a weak constraint, indicating the recommended execution order, such as "SearchItem → AddToCart", but it doesn't mean "SearchItem" must be executed before "AddToCart", and is more used for execution path optimization. In the skill evolution stage, the system proposes skill combination suggestions based on path structures and skill libraries in SPG by LLM, generating composite skills (such as combining "Login", "SearchItem", "AddToCart" to construct "OneClickPurchase"), adding them as new nodes to `nodes`, and updating `edges` to establish new structural relationships, thereby achieving automatic evolution and calling optimization of skills, significantly improving the agent's generalization ability and execution efficiency of complex tasks. For details, please refer to SkillEvolvingStage in Section 2.3 SkillGenesis.

SPG is represented in JSON:

```json
{
  "nodes": {
    "enter_specific_forum_section": {
      "type": "atomic",
      "description": "Navigate to the specific forum section"
    },
    "sort_forum_by_new": {
      "type": "atomic",
      "description": "Sort the forum posts by newest"
    },
    "go_to_user_profile_and_open_comments": {
      "type": "atomic",
      "description": "Open the latest post author's profile and comments"
    },
    "count_controversial_comments": {
      "type": "atomic",
      "description": "Count comments with more downvotes than upvotes"
    }
  },
  "edges": {
    "enter_specific_forum_section": [
      {
        "target": "sort_forum_by_new",
        "type": "invokes"
      }
    ],
    "go_to_user_profile_and_open_comments": [
      {
        "target": "count_controversial_comments",
        "type": "depends_on"
      }
    ]
  }
}
```

## **Weakness 4: Table 2 and Section 5.2**

The results in Table 2 and Section 5.2 come from our designed Scaled-Up Browsing Tasks, with task lists and checkpoint definitions detailed in Supplementary Material D Scaled-Up Tasks. These tasks run on five websites (Shopping, Admin, Reddit, GitLab, Map). The WebArena-lite benchmark mainly features isolated, single-task scenarios, such as adding a product to the shopping cart. However, in real life, people usually need to complete a series of such tasks consecutively, such as adding multiple related items (e.g., coffee and cups) to the shopping cart together before checkout. Such browsing requests result in extremely long task durations and sometimes involve repetitive intermediate processes. Through these extended tasks with long time spans and multiple sub-tasks, we can demonstrate the efficiency and effectiveness of our reusable skills. Therefore, unlike the atomic tasks in Table 1, extended tasks are closer to real user behavior by linking multiple related sub-goals into a long-chain task. Thus, Table 2 and Table 1 are complementary rather than duplicate benchmarks.

To accurately evaluate the execution effectiveness of these complex tasks, we borrowed from Xu et al. (2024)'s approach and designed intermediate checkpoints for each task, calculating success rates based on checkpoint completion ratios. This design can measure the agent's multi-stage stability in long-chain tasks at a finer granularity, so SkillEvolve + SPG achieving 100% success rate in some tasks doesn't mean the tasks are simple, but indicates its ability to complete all sub-goals in long-chain tasks completely and stably.

Additionally, extended tasks are particularly suitable for evaluating long-term decision-making capabilities. On one hand, high success rates indicate the agent's ability to stably complete all sub-tasks in long-chain tasks; on the other hand, step reduction is another important manifestation of long-term decision-making capability: the ability to reasonably decompose complex tasks, effectively reuse learned skills, and reduce redundant exploration, thereby completing tasks with optimal paths. If the agent lacks long-term planning capabilities, it often repeatedly tries or deviates from goals in intermediate stages, leading to increased steps or task failure. SkillEvolve significantly reduces steps while maintaining high success rates in extended tasks, indicating that it can not only complete tasks but also complete them in a more efficient and organized manner, truly reflecting the improvement of long-term decision-making capabilities. On this point, the short tasks in Table 1 cannot fully reflect "overall planning efficiency across sub-tasks", while the extended tasks in Table 2 can clearly distinguish the capability differences of different methods in task decomposition, path planning, and skill reuse. SkillEvolve achieving both higher success rates and fewer steps in extended tasks reflects its long-term decision-making advantages in real complex web scenarios.

## **Framework Question 6: How is SkillGenesis used together with WebGRPO?**

The combination of SkillGenesis and WebGRPO is reflected in the two-stage design of the SkillEvo framework: First, WebGRPO trains a strategy model with high-quality reasoning and web interaction capabilities through trajectory-level reinforcement learning to solve multi-step tasks in complex web environments; Subsequently, in the SkillGenesis stage, this strategy model is loaded, and through three sub-stages of SkillProposal, SkillGenesis, and SkillEvolving, reusable skills are generated and evolved, building a skill library and SPG, thereby significantly improving the model's execution efficiency and generalization capability on benchmark tasks.

## **Framework Question 9: When predicting intermediate reasoning rewards, are all intermediate reasoning statements in the entire trajectory input to the model together?**

When predicting intermediate reasoning rewards, we do not input all intermediate reasoning statements in the entire trajectory to the model for unified evaluation, but evaluate each reasoning statement individually and independently. The specific prompt used for this evaluation has been detailed in Figure 10 *Reasoning Quality Evaluation Prompt* in Supplementary Material F.

## **Experimental Question 1: What values are the relative improvement numbers in Table 1 based on?**

Thank you for the reviewer's comment. The "relative improvement" indicators (green rows) listed in Table 1 are calculated relative to WebGRPO without Skills. This has been clearly stated in Section 4 "Main Results" (lines 246–248, 250-252), for example, lines 246–248 point out that in GitLab and Reddit environments, compared to WebGRPO without skill reuse, success rates improved by 18.0% and 14.0% respectively. We will supplement explanations in table annotations to further enhance clarity.

## **Experimental Question 4: Clarification of experimental data sources in Section 5.2**

The "1.3" and "4.9 steps" mentioned in Section 5.2 refer to the range of operation steps reduced by SkillEvolve in different tasks after introducing SPG compared to configurations without SPG. According to table data, the minimum step reduction value is 1.3 steps, occurring in the Map task (from 4.5 steps to 3.2 steps); the maximum step reduction value is 4.9 steps, occurring in the GitLab task (from 20.7 steps to 15.8 steps). Additionally, the Admin task reduced steps by 1.5 (from 10.8 to 9.3), and the Reddit task reduced steps by 3.4 (from 9.9 to 6.5).

## **Question:Notations and typos**

Thank you for your careful check. We will correct all the typos, inconsistent notations, and figure/table formatting issues in the revision. We sincerely appreciate your detailed feedback.

---

**We hope the above supplements and clarifications can resolve your concerns**

# Rebuttal to Reviewer 8pAH

We thank the reviewer for the careful reading and valuable suggestions. The following responses address each **weakness/question** raised by the reviewer.

## **Weakness 1: Questionable statistical significance of results**

We thank the reviewer for their attention to statistical significance issues. We chose to evaluate using the 165 high-quality subset tasks from the WebArena benchmark mainly based on two considerations: First, there are some annotation errors and unclear evaluation standards in the original dataset, which may affect the fairness and credibility of evaluation. In contrast, this subset has been manually reviewed and corrected, ensuring evaluation result accuracy and consistency to a greater extent; Second, considering the resource and time costs of conducting experiments in large-scale interactive environments, we chose this subset as a compromise solution while ensuring evaluation quality.

## **Weakness 2: Questionable task quality in skill proposal stage**

Thank you for your valuable opinion. Regarding the concern about "questionable task quality in skill proposal stage", the Skill Proposal Prompt we provided in Supplementary Material F has been specially designed: this prompt sets type limitations, interaction step limits (≤10 steps), and three priorities of "create-edit-retrieve" for each candidate task, while mandatorily prohibiting single-click and structurally fragile skills, ensuring task coverage and necessary complexity from the source; Subsequently, a three-dimensional scoring system of "practicality + generalizability + steps" is used to quantitatively evaluate skill candidates, automatically eliminating samples that are too coarse or too fine-grained. We believe ideal skill granularity should have clear boundaries, contain multi-step effective interactions, and possess good reusability and combination potential. It's worth supplementing that the system will immediately detect whether new skills duplicate existing skills before adding them to the task pool and automatically eliminate duplicates, thereby ensuring skill library diversity and high quality. With GPT-4o's excellent instruction-following capability, these mechanisms are executed stably and reliably in practice. Regarding dynamic changes in skill granularity, our framework design indeed considers this point. The Skill Evolving stage is specifically responsible for skill combination and evolution, combining existing fine-grained skills into more complex composite skills, with the system triggering skill combination every Q=20 iterations.

## **Weakness 3: Necessity of skill evolution stage**

In the complete 165 test tasks, we calculated the coverage rate, call frequency, and task complexity distribution of skill calls. After 160 exploration iterations and triggering composite skill generation every 20 Genesis iterations, the final skill library size reached 134 skills, including 88 basic skills and 46 composite skills (composite skills accounting for 34%). In the test set, 86 skills were actually called (coverage rate 64%), including 67 basic skills and 19 composite skills.

The total number of skill calls was 193 (approximately 1.17 times/task), with basic skill calls 141 times (73%) and composite skill calls 52 times (27%). Although composite skills are fewer in number, their average call frequency reaches 2.74 times/skill, higher than basic skills' 2.10 times/skill, showing higher reuse rates.

Statistics by task-level skill call numbers: 33% of tasks (55) can be completed entirely relying on atomic operations without calling any skills; 36% of tasks (60) call only 1 skill; tasks calling 2 skills and 3 skills account for 16% (27) and 8% (13) respectively; high-complexity tasks calling 4 skills account for 6% (10 tasks). Most simple tasks can be completed relying on atomic operations or a few basic skills, while high-complexity tasks (≥3 skill calls) significantly depend on skill calls, contributing 41% of all call times.

**GLM-4-9B (9B)**

| Training Configuration | Reddit | GitLab | CMS  | Map  | OSS  | Avg  |
| ---------------------- | ------ | ------ | ---- | ---- | ---- | ---- |
| WebGRPO (no skills)    | 62.3   | 52.1   | 52.9 | 45.3 | 46.9 | 50.3 |
| WebGRPO (Base-only)    | 66.8   | 55.8   | 54.1 | 47.5 | 48.7 | 54.6 |
| WebGRPO (Base + Compose) | 71.2   | 61.2   | 59.2 | 50.6 | 51.5 | 57.6 |

**Llama-3.1-8B (8B)**

| Training Configuration | Reddit | GitLab | CMS  | Map  | OSS  | Avg  |
| ---------------------- | ------ | ------ | ---- | ---- | ---- | ---- |
| WebGRPO (no skills)    | 65.8   | 51.3   | 59.1 | 42.3 | 44.5 | 51.8 |
| WebGRPO (Base-only)    | 69.7   | 55.2   | 61.3 | 44.1 | 49.4 | 55.9 |
| WebGRPO (Base + Compose) | 75.8   | 60.2   | 65.3 | 48.1 | 53.9 | 60.4 |

Experimental results show that GLM-4-9B's average success rate improved from 50.3% with "no skill library" to 54.6% with only basic skills (+4.3 percentage points), and further improved to 57.6% (+3.0 percentage points) after adding composite skills; Llama-3.1-8B's average success rate improved from 51.8% → 55.9% (+4.1 percentage points), then to 60.4% (+4.5 percentage points). It can be seen that composite skills alone contributed about 40–50% of the overall gain, with improvement magnitude equal to or higher than basic skills. Although composite skills only account for 34% of the skill library (46/134) and 27% of calls (52/193), their average reuse frequency of 2.74 times/skill is significantly higher than basic skills (2.10 times/skill) and plays a key role in high-complexity tasks. Therefore, the skill evolution stage (generation and use of composite skills) is a necessary mechanism to ensure success rate improvement in complex tasks.

## **Question 1: Additional costs introduced by multi-dimensional reward design**

The main additional costs come from GPT-4o calls, where RXERM two-stage reward evaluation accumulates approximately 248M tokens (execution reward approximately 220M tokens, reasoning reward approximately 28M tokens). Additional overhead mainly comes from SkillGenesis bringing approximately 42M tokens (skill proposal one-time approximately 64k tokens, skill verification approximately 42M tokens, skill evolution approximately 50k tokens).

## **Question 2: Explanation of reward combination in Figure 5**

Regarding how to evaluate rewards, please refer to Figures 10 and 11 in Supplementary Material F. To make the effectiveness of reward combination in Figure 5 more intuitive, we use the actual task "Tell me the number of reviews that our store received by far that mention term 'best'" as an example to show each reward's evaluation dimensions and their impact on model behavior.

In the first round (Round 1) interaction during training, the model generated the following output:

<think>

Let me help you find the number of reviews mentioning "best". The current page is the dashboard, and I don't see a direct entry to reviews. We need to navigate to the review area. According to the menu structure, reviews are generally under the MARKETING section. Let me click the MARKETING menu: click("226") After clicking, we should see a submenu containing "reviews". From there we can search for reviews related to "best". This way we can more accurately get the number of reviews mentioning this term.

</think>

<answer>

do(action="Click", element="226")

</answer>

In this round, the model has not yet completed the final task but has shown reasonable intermediate planning and effective behavior. We analyze each type of reward as follows:

1. Execution reward Rexec: This round has not successfully obtained the review count, so the execution reward is recorded as 0.

2. Reasoning reward Rreason: This reward is evaluated by a large language model (GPT-4o) through a specialized scoring template for the logic, task relevance, webpage state understanding, and intermediate step clarity of the `<think>` section. The scoring result for the above output is 0.975, with the following explanation: This reasoning is very close to the task goal, shows good understanding of the Magento backend structure, and reasonably plans steps to enter the "Reviews" module from the "MARKETING" menu to search for the keyword "best", with clear intermediate goals and strong logical consistency. The only slight deduction is in webpage state understanding: although inferring that "reviews" are under "MARKETING" is reasonable, it doesn't directly confirm that the "Reviews" menu item actually exists, but makes a prediction based on experience. Therefore, 0.1 was deducted for "Understanding of Webpage State", with full marks for other items, resulting in a final score of 0.975.

3. Format reward: This output strictly follows our set `<think>...</think><answer>...</answer>` format structure, conforming to structural constraints, so the format reward score is 1.

This example reflects that in actual training processes, many trajectories, although not yet completing tasks, possess high-quality reasoning processes and reasonable action choices. If we only rely on final execution success for reward evaluation (i.e., only using Rexec), such high-potential samples would be considered invalid and difficult to provide effective gradients for strategy learning. By introducing Rreason and Rformat, we can retain and strengthen these "reasoning effective, execution incomplete" intermediate trajectories, thereby providing dense, stable supervision signals in early training, significantly alleviating "sparse reward" and "reasoning degradation" problems in multi-round web tasks.

## **Question 3: How many tasks were synthesized for training?** How does agent performance change as the number of training samples increases?

Approximately 4k original task instances were synthesized for training, with 160 used for SkillGenesis exploration. Performance rapidly improves with sample size initially, then shows diminishing returns and eventually reaches saturation.

## **Question 4: Analysis of scaled activities in Section 5.2**

Thank you for the reviewer's detailed observation. Indeed, "SkillEvolver" is first formally named in Section 5.2, although not explicitly used in earlier text, it does refer to our proposed method as a whole. We acknowledge some errors and unclear expressions in our statements and hereby clarify: SkillEvolver includes both training and skill learning core stages.

To analyze the respective contributions of these two parts to final results, we supplemented the following ablation experimental results:

- SkillEvolver (no SPG & no skills): 100.0% / 13.8, 92.2% / 12.7, 57.0% / 10.9, 56.2% / 23.5, 100.0% / 5.9
- SkillEvolver (no SPG & with skills): 100.0% / 10.2, 93.0% / 10.8, 60.0% / 9.9, 57.8% / 20.7, 100.0% / 4.5
- SkillEvolver (with SPG & with skills): 100.0% / 6.1, 96.2% / 9.3, 64.7% / 6.5, 60.1% / 15.8, 100.0% / 3.2

From the results, it can be seen that skill learning significantly reduces steps and improves success rates compared to no skills, while further introducing SPG again improves success rates and greatly reduces steps on this basis. It should be specially noted here: fewer steps represent higher efficiency, so "w/ SPG" is not performance degradation but achieving higher efficiency and success rates. The reviewer may have misunderstood step reduction as performance decline when reading Table 2, so we clarify this.

# Rebuttal to Reviewer c6aF

We thank the reviewer for the careful reading and valuable suggestions. The following responses address each **weakness/question** raised by the reviewer.

## Weakness 1: Regarding WebGRPO

**Regarding WebGRPO:** Although from Table 1 and Figure 6, WebGRPO seems inferior to WebRL when not using dual uncertainty active learning, it should be noted that **the dual uncertainty mechanism is specifically designed for problems existing in the GRPO optimization process and cannot be directly applied to WebRL**. Specifically, GRPO does not rely on value networks. It is prone to this problem during training: when all rollout rewards for the same task instance are similar (e.g., all successful or all failed), intra-group advantages degrade to zero overall, causing policy gradients to become 0, thereby reducing gradient magnitude and increasing batch gradient sensitivity to noise, thus reducing sampling efficiency. The dual uncertainty mechanism is designed to solve this problem: by simultaneously considering execution uncertainty and reasoning uncertainty to stabilize advantage signals. In contrast, WebRL adopts a standard **PPO/actor-critic framework** with value networks and optimizes based on **action-level rewards**, which does not inherently have intra-group advantage collapse problems. Therefore, directly migrating the dual uncertainty mechanism to WebRL lacks theoretical compatibility and is difficult to achieve the same effects. For the same reason, RXERM (Reasoning & Execution Reward Model) is also a trajectory-level reward modeling method designed for WebGRPO to provide more fine-grained reasoning and execution reward signals for GRPO, rather than simply replacing WebRL's reward function. It can be said that RXERM and the dual uncertainty mechanism together constitute an overall solution specifically for stabilizing **GRPO trajectory-level strategy optimization**, rather than a general plugin for PPO-type algorithms.

## Weakness 2: Related costs

The main additional costs come from GPT-4o calls, where RXERM two-stage reward evaluation accumulates approximately 248M tokens (execution reward approximately 220M tokens, reasoning reward approximately 28M tokens). Additional overhead mainly comes from SkillGenesis bringing approximately 42M tokens (skill proposal one-time approximately 64k tokens, skill verification approximately 42M tokens, skill evolution approximately 50k tokens).

## Weakness 3: Training data

The dataset used for SFT (Supervised Fine-Tuning) training is a human-annotated demonstration dataset provided by WebArena-Lite. This dataset was carefully designed and generated by **5 authors** based on approximately **40 task prototypes**, ultimately constructing **1,186 high-quality expert trajectories**. These demonstrations cover 5 real websites in WebArena-Lite with diverse task types and clear operation steps. For training data in the reinforcement learning phase, the Baselines section in Section 3.1 of the main text has clear descriptions. We adopted automatic task generation, specifically, instructions come from a skill proposer. This module automatically generates numerous task instances based on initial webpage states (including 5 websites) during training, forming a task instance pool. Specific details can be found in *Skill Proposal Stage* in Section 2.3 *SkillGenesis*.

## Weakness 4: Innovation

We disagree with viewing this work as a simple combination of WebRL and ASI. Although WebRL proposes the idea of combining curriculum and RL, it adopts PPO-based RL, requiring additional critic models and complex training mechanisms, mainly optimizing action-level rewards, making it difficult to solve long-sequence reasoning and sparse reward problems; Although ASI abstracts skills as executable programs and incorporates them into action spaces to improve efficiency, it relies on static prompt-driven skill updates, making its adaptability and generality to new tasks and diverse web environments poor. Addressing these shortcomings, we propose a dual-stage training framework including: trajectory-level GRPO (WebGRPO) extends GRPO to complete reasoning-interaction trajectories and directly optimizes trajectory-level strategies; RXERM dual-stage reward mechanism decouples reasoning and execution evaluation, systematically solving the key challenge that reasoning capabilities are difficult to naturally emerge; dual uncertainty active learning improves sample information content and training stability; and SkillGenesis skill evolution framework (SPG-driven) dynamically extracts, combines, and evolves skills to enhance cross-task generalization. This "trajectory-level optimization + skill-level evolution" closed-loop design balances stability and generalization with significant effects, achieving absolute success rate improvement of 18% on WebArena-Lite (Llama3.1+WebGRPO(+Skills) 60.4% vs. Llama3.1+WebRL 42.4%), fully demonstrating the novelty and effectiveness of the method.

## Question 1: R_reason scoring

Thank you for the reviewer's detailed question. Regarding line 145, we indeed score each `<think>` reasoning unit independently and average them to calculate the overall R_reason.

## Question 2: Training data

The dataset used for SFT (Supervised Fine-Tuning) training is a human-annotated demonstration dataset provided by WebArena-Lite. This dataset was carefully designed and generated by **5 authors** based on approximately **40 task prototypes**, ultimately constructing **1,186 high-quality expert trajectories**. These demonstrations cover 5 real websites in WebArena-Lite with diverse task types and clear operation steps. For training data in the reinforcement learning phase, the Baselines section in Section 3.1 of the main text has clear descriptions. We adopted automatic task generation, specifically, instructions come from a skill proposer. This module automatically generates numerous task instances based on initial webpage states (including 5 websites) during training, forming a task instance pool. Specific details can be found in *Skill Proposal Stage* in Section 2.3 *SkillGenesis*.

## Question 3: Regarding SkillGenesis

In the complete 165 test tasks, we calculated the coverage rate, call frequency, and task complexity distribution of skill calls. After 160 exploration iterations and triggering composite skill generation every 20 Genesis iterations, the final skill library size reached 134 skills, including 88 basic skills and 46 composite skills (composite skills accounting for 34%). In the test set, 86 skills were actually called (coverage rate 64%), including 67 basic skills and 19 composite skills.

The total number of skill calls was 193 (approximately 1.17 times/task), with basic skill calls 141 times (73%) and composite skill calls 52 times (27%). Although composite skills are fewer in number, their average call frequency reaches 2.74 times/skill, higher than basic skills' 2.10 times/skill, showing higher reuse rates.

Statistics by task-level skill call numbers: 33% of tasks (55) can be completed entirely relying on atomic operations without calling any skills; 36% of tasks (60) call only 1 skill; tasks calling 2 skills and 3 skills account for 16% (27) and 8% (13) respectively; high-complexity tasks calling 4 skills account for 6% (10 tasks). Most simple tasks can be completed relying on atomic operations or a few basic skills, while high-complexity tasks (≥3 skill calls) significantly depend on skill calls, contributing 41% of all call times.

In this experiment, skill set access and filtering are completed through directly listing skill functions in the Prompt. Specifically, 134 skill functions are divided into 5 batches, with approximately 27 functions per batch, embedded in the `<functions>...</functions>` block of the Prompt. The model judges whether skills are relevant based on task descriptions within each batch Prompt and outputs JSON results containing `step by step reasoning` and `function names`. Subsequently, results from 5 batches are merged and deduplicated to obtain candidate skill sets, then a ranking Prompt is run to select Top-15 skill functions by relevance, finally injecting these skills into the strategy model Prompt for task execution. Compared to database retrieval, this method is more direct and interpretable. On one hand, the skill library is relatively small, and directly listing skills in batched Prompts is sufficient to cover all skills without relying on external retrieval systems; On the other hand, database or vector retrieval may miss key skills due to inaccurate recall, while directly listing skills ensures completeness and transparency. This method can reduce experimental variables, ensuring that filtering logic is entirely completed by the model within the Prompt, facilitating verification of the impact of skill filtering on task execution performance.

```sql
A.8 Skill Retrieval

You are provided with a list of Python functions that represent action shortcuts available on a website (in addition to basic actions like click, type, hover, select option, etc.).

<functions>
{function space}
</functions>

Your task is:
{repr task}

Please analyze each function and determine whether it is useful for the task.  
Finally, output a JSON in the following format:
{
  "step by step reasoning": "Explain your reasoning process",
  "function names": ["function_name_1", "function_name_2", "function_name_3"]
}
```

```python
Now you have a list of candidate functions :

{candidate functions list}

Your task is:
{repr task}

Please rank these functions based on their relevance to the task 
and return the Top-15 most relevant functions.

Output JSON in the following format:
{
  "step by step reasoning": "Explain how you determined the ranking",
  "function names": ["Top1_function_name", "Top2_function_name", ..., "Top15_function_name"]
}
```